{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9316c2a-fa27-4164-bf3a-2fdd8c976d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X: 0\n",
      "Missing values in y: 0\n",
      "X_train shape: (900, 384)\n",
      "y_train shape: (900,)\n",
      "Best Perceptron Params: {'penalty': 'l2', 'max_iter': 1000, 'eta0': np.float64(0.001291549665014884)}\n",
      "Perceptron Test Score: 0.4247787610619469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.17      0.09         6\n",
      "           1       0.48      0.34      0.40        29\n",
      "           2       0.44      0.44      0.44        72\n",
      "           3       0.41      0.39      0.40        61\n",
      "           4       0.52      0.49      0.51        47\n",
      "           5       0.43      0.55      0.48        11\n",
      "\n",
      "    accuracy                           0.42       226\n",
      "   macro avg       0.39      0.40      0.39       226\n",
      "weighted avg       0.45      0.42      0.43       226\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset from Excel file\n",
    "def load_data(file_path, sheet_name=0):\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    print(f\"Data loaded successfully with shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "# Hyperparameter tuning for Perceptron\n",
    "def tune_perceptron(X_train, y_train):\n",
    "    perceptron = Perceptron()\n",
    "    param_dist = {\n",
    "        'penalty': ['l2', None],  # Valid options for regularization\n",
    "        'eta0': np.logspace(-4, 1, 10),  # Learning rate values\n",
    "        'max_iter': [1000, 2000, 3000]\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(perceptron, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search\n",
    "\n",
    "# Hyperparameter tuning for MLP\n",
    "def tune_mlp(X_train, y_train):\n",
    "    mlp = MLPClassifier()\n",
    "    param_dist = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': np.logspace(-4, 1, 10),\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('training_mathbert.xlsx')  # Ensure the file path is correct\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Target\n",
    "\n",
    "# Ensure target labels are integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in X: {np.isnan(X).sum()}\")\n",
    "print(f\"Missing values in y: {np.isnan(y).sum()}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Debugging: Check shape and initial values\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Tune Perceptron\n",
    "perceptron_search = tune_perceptron(X_train, y_train)\n",
    "print(f\"Best Perceptron Params: {perceptron_search.best_params_}\")\n",
    "print(f\"Perceptron Test Score: {perceptron_search.score(X_test, y_test)}\")\n",
    "\n",
    "# Classification report for Perceptron\n",
    "y_pred_perceptron = perceptron_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_perceptron))\n",
    "\n",
    "# Tune MLP\n",
    "mlp_search = tune_mlp(X_train, y_train)\n",
    "print(f\"Best MLP Params: {mlp_search.best_params_}\")\n",
    "print(f\"MLP Test Score: {mlp_search.score(X_test, y_test)}\")\n",
    "\n",
    "# Classification report for MLP\n",
    "y_pred_mlp = mlp_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12adc610-7667-48d0-80d3-6c93165f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1126, 384)\n",
      "y shape: (1126,)\n",
      "X: [[-0.08992592  0.34387389  0.17638168 ...  0.10559644  0.1964383\n",
      "   0.11719853]\n",
      " [ 0.30326107  0.08492954  0.04736905 ... -0.02969474  0.33597666\n",
      "  -0.19753899]\n",
      " [-0.27429068  0.21680066  0.02911038 ... -0.08576754  0.5129559\n",
      "   0.02333383]\n",
      " ...\n",
      " [-0.29696792 -0.20464683  0.07746858 ...  0.11653966  0.19947569\n",
      "  -0.09766734]\n",
      " [-0.07764415  0.16842389 -0.18984014 ... -0.16399319  0.06891123\n",
      "   0.0273101 ]\n",
      " [-0.22067161  0.30338642  0.34436339 ...  0.18375197  0.11251207\n",
      "  -0.08536078]]\n",
      "y: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.25       0.25\n",
      " 0.375      0.5        0.5        0.5        0.5        0.5\n",
      " 0.5        0.5        0.5        0.5        0.5        0.5\n",
      " 0.5        0.5        0.5        0.5        0.5        0.5\n",
      " 0.5        0.5        0.625      0.75       0.75       0.75\n",
      " 0.75       0.875      0.875      1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.125      1.125      1.125      1.125      1.125\n",
      " 1.15625    1.1875     1.25       1.25       1.25       1.25\n",
      " 1.25       1.25       1.25       1.25       1.25       1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.5\n",
      " 1.5        1.5        1.5        1.5        1.5        1.5\n",
      " 1.5        1.5        1.5        1.5        1.5        1.5\n",
      " 1.5        1.5        1.5        1.5        1.5        1.5625\n",
      " 1.5625     1.5625     1.625      1.625      1.65       1.75\n",
      " 1.75       1.75       1.75       1.75       1.75       1.75\n",
      " 1.75       1.75       1.75       1.75       0.5        1.\n",
      " 1.         0.5        1.         1.75       1.75       1.75\n",
      " 1.         1.         0.5        1.875      1.875      1.875\n",
      " 1.875      1.5        1.875      1.875      2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.125      2.125      2.125      2.\n",
      " 2.         2.         2.         2.         2.         2.\n",
      " 2.         2.         2.         2.         2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.25       2.25       2.25\n",
      " 2.25       2.25       2.25       2.3125     2.3125     2.33333333\n",
      " 2.33333333 2.33333333 2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.375      2.375      2.375      2.375      2.375\n",
      " 2.375      2.4375     2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.5        2.5        2.5        2.5        2.5        2.5\n",
      " 2.54166667 2.5625     2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.625\n",
      " 2.625      2.625      2.625      2.625      2.625      2.65625\n",
      " 2.66071429 2.66666667 2.6875     2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.75\n",
      " 2.75       2.75       2.75       2.75       2.75       2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.875      2.875      2.875      2.875      2.875      2.875\n",
      " 2.90625    2.91666667 2.9375     3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.         3.         3.\n",
      " 3.         3.         3.         3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.125      3.125      3.125\n",
      " 3.125      3.125      3.125      3.21875    3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.25\n",
      " 3.25       3.25       3.25       3.25       3.25       3.3125\n",
      " 3.3125     3.34375    3.375      3.375      3.375      3.375\n",
      " 3.375      3.375      3.375      3.375      3.375      3.375\n",
      " 3.375      3.375      3.375      3.375      3.41666667 3.41666667\n",
      " 3.45833333 3.45833333 3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.5        3.5\n",
      " 3.5        3.5        3.5        3.5        3.59375    3.625\n",
      " 3.625      3.625      3.625      3.625      3.625      3.625\n",
      " 3.625      3.625      3.625      3.625      3.625      3.625\n",
      " 3.625      3.625      3.625      3.625      3.625      3.625\n",
      " 3.625      3.625      3.6875     3.6875     3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.75       3.75       3.75       3.75\n",
      " 3.75       3.75       3.875      3.875      3.875      3.875\n",
      " 3.875      3.875      3.875      3.875      3.875      3.875\n",
      " 3.875      3.875      3.875      3.875      3.875      3.875\n",
      " 3.875      3.875      3.875      3.875      3.875      3.875\n",
      " 3.9375     3.9375     4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.         4.\n",
      " 4.         4.         4.         4.         4.125      4.125\n",
      " 4.125      4.125      4.125      4.125      4.125      4.125\n",
      " 4.125      4.125      4.125      4.125      4.125      4.125\n",
      " 4.125      4.125      4.125      4.125      4.125      4.125\n",
      " 4.125      4.125      4.125      4.16666667 4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.25       4.25       4.25       4.25       4.25       4.25\n",
      " 4.33333333 4.375      4.375      4.375      4.375      4.375\n",
      " 4.375      4.375      4.375      4.375      4.375      4.375\n",
      " 4.375      4.375      4.375      4.375      4.375      4.375\n",
      " 4.375      4.375      4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.5        4.5        4.5        4.5        4.5        4.5\n",
      " 4.625      4.625      4.625      4.625     ]\n",
      "Missing values in X_train: 0\n",
      "Missing values in y_train: 0\n",
      "Training SVC...\n",
      "Training DecisionTreeClassifier...\n",
      "Training RandomForestClassifier...\n",
      "Training AdaBoostClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GaussianNB...\n",
      "Training GradientBoostingClassifier...\n",
      "Classifier: SVM\n",
      "Accuracy: 0.40707964601769914\n",
      "F1 Score: 0.3753058367758543\n",
      "ROC AUC: 0.7888481685973295\n",
      "\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Accuracy: 0.36283185840707965\n",
      "F1 Score: 0.36265343218200957\n",
      "ROC AUC: 0.5697276049978968\n",
      "\n",
      "\n",
      "Classifier: Random Forest\n",
      "Accuracy: 0.46017699115044247\n",
      "F1 Score: 0.44877241129506834\n",
      "ROC AUC: 0.7567482877774027\n",
      "\n",
      "\n",
      "Classifier: AdaBoost\n",
      "Accuracy: 0.28761061946902655\n",
      "F1 Score: 0.28114902316435814\n",
      "ROC AUC: 0.573377318573513\n",
      "\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Accuracy: 0.35398230088495575\n",
      "F1 Score: 0.36538420243380343\n",
      "ROC AUC: 0.6601524990452755\n",
      "\n",
      "\n",
      "Classifier: GradientBoosting\n",
      "Accuracy: 0.4424778761061947\n",
      "F1 Score: 0.4220793516681649\n",
      "ROC AUC: 0.7180483426229795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Fit and evaluate a classifier\n",
    "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Fit the classifier and evaluate its performance.\"\"\"\n",
    "    print(f\"Training {clf.__class__.__name__}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    # Try to calculate ROC AUC if classifier supports predict_proba or decision_function\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        try:\n",
    "            y_prob = clf.predict_proba(X_test)\n",
    "            metrics['ROC AUC'] = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "        except ValueError:\n",
    "            metrics['ROC AUC'] = \"N/A - issue with probabilities\"\n",
    "    else:\n",
    "        metrics['ROC AUC'] = \"N/A - no predict_proba\"\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Compare different classifiers\n",
    "def compare_classifiers(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Compare multiple classifiers and tabulate their results.\"\"\"\n",
    "    classifiers = {\n",
    "        'SVM': SVC(probability=True),  # Ensure SVM has probability enabled\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'GradientBoosting': GradientBoostingClassifier()\n",
    "    }\n",
    "\n",
    "    # Evaluate each classifier and store results\n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        results[name] = evaluate_classifier(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"Classifier: {name}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Main function for A3 (Classifier Comparison)\n",
    "# Load data\n",
    "data = pd.read_excel('training_mathbert.xlsx')\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values  # Target\n",
    "\n",
    "# Debugging: Check shapes and first few rows of data\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X:\", X[:1000])\n",
    "print(\"y:\", y[:1000])\n",
    "\n",
    "# Ensure target labels are integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Debugging: Check for missing values\n",
    "print(f\"Missing values in X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"Missing values in y_train: {np.isnan(y_train).sum()}\")\n",
    "\n",
    "# Compare classifiers\n",
    "compare_classifiers(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c88da-7810-489d-9697-7c501bc68f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
